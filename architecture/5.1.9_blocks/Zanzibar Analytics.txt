Zanzibar Analytics - System Architecture Design (v1.0)
CTO Briefing: This document outlines the architectural design for the Zanzibar Analytics trading system. It details the core components, their interactions, data flows, technology choices, and key design principles necessary for building a professional, scalable, and robust application suitable for quantitative analysis and signal generation, with eventual integration for execution via prop firm platforms.
1. Guiding Principles:
	•	Modularity: Components should be distinct, independently testable, and replaceable with minimal impact on others.
	•	Scalability: Design for potential increases in data volume (more symbols, higher frequency), computational load (complex models), and feature scope.
	•	Data Integrity: Prioritize clean, validated, and accurately timestamped data throughout the system.
	•	Testability: Build with unit, integration, and end-to-end testing in mind from the start.
	•	Resilience: Implement fault tolerance and graceful degradation where possible.
	•	Clarity & Maintainability: Code should be well-documented, follow consistent patterns, and be understandable by the team.
	•	ADHD-Aware Design: Ensure clear interfaces, focused outputs, and mechanisms for managing cognitive load (e.g., clear dashboards, concise reporting).
2. High-Level System Architecture:
graph TD     subgraph External Sources         A[Data Feeds (Sierra Chart - Denali/Crypto)]         B[Broker API (ACAP - Future/Status)]         C[PMS Input (Patryk - Text/API)]         D[Prop Firm Platform (FTMO MT5 - Execution)]     end      subgraph Zanzibar Analytics Core System         subgraph Ingestion Layer             I1(Tick/DOM Ingestor - SCID/CSV)             I2(PMS Parser - NLP/JSON)             I3(Broker API Connector - FIX/REST)         end          subgraph Data Management Layer             DM1(Time-Series DB - Tick/DOM/Features)             DM2(Metadata/Narrative DB - PMS/Config)             DM3(Data Validation & Cleaning)         end          subgraph Analysis Engine             AE1(Liquidity Engine - Sweeps/Imbalance)             AE2(Structure Engine - Wyckoff/SMC/POI)             AE3(Feature Engineering)             AE4(Correlation/Regime Analysis)         end          subgraph ML & Validation Engine             ML1(Model Training Pipeline - Boosting/Linear)             ML2(Statistical Validation - IC/DSR/BayesSR)             ML3(PMS Effectiveness Tracker)             ML4(Model Interpretation - SHAP)         end          subgraph Risk & Portfolio Engine             RP1(Volatility Forecaster - GARCH/ML)             RP2(Position Sizer - Kelly)             RP3(Risk Calculator - VaR/CVaR)         end          subgraph Orchestration & Control             OC1(Master Orchestrator - Copilot)             OC2(Configuration Management)             OC3(Logging & Monitoring)             OC4(Scheduling)         end          subgraph Execution & Output Layer             EO1(Signal Aggregator & Filter)             EO2(Execution Adapter - MT5 Interface)             EO3(Reporting & Dashboard Interface)         end     end      subgraph User Interface / External Tools         UI1(Control Dashboard / CLI)         UI2(Bookmap - Visualization)         UI3(Sierra Chart - Platform)     end      %% Data Flows     A -- Tick/DOM Data --> I1     C -- Raw Narrative --> I2     B -- Market/Status Data --> I3      I1 -- Processed Tick/DOM --> DM1     I2 -- Structured Narrative (JSON) --> DM2     I3 -- Broker Data --> DM1/DM2      DM1 -- Time Series Data --> AE1 & AE2 & AE3 & AE4     DM2 -- Narrative/Config Data --> OC1 & AE2 & ML3     DM3 -- Validated Data --> DM1 & DM2      AE1 & AE2 & AE3 & AE4 -- Features/Metadata --> DM1 & ML1     ML1 -- Trained Models --> ML2 & EO1     ML2 -- Validation Metrics --> OC3 & EO3     ML3 -- PMS Scores --> OC3 & EO3     ML4 -- Interpretations --> EO3      DM1 -- Price/Vol Data --> RP1 & RP2 & RP3     RP1 & RP2 & RP3 -- Risk/Sizing Params --> EO1     AE1 & AE2 & ML1 -- Raw Signals --> EO1      OC1 -- Control Flow --> I1 & I2 & I3 & AE1 & AE2 & AE3 & AE4 & ML1 & RP1 & RP2 & RP3 & EO1 & EO2 & OC3     OC2 -- Config --> All Components     OC3 -- Logs/Metrics --> UI1      EO1 -- Final Trade Signal --> EO2     EO2 -- Execute Order --> D     D -- Execution Status --> EO2     EO2 -- Fill/Status Data --> DM1/DM2 & OC3      DM1 & DM2 & OC3 & ML4 -- Data for UI --> EO3     EO3 -- Formatted Data/Reports --> UI1      A -- Direct Feed --> UI3 & UI2 %% User can view raw feed in SC/Bookmap  
3. Component Breakdown & Key Modules (Illustrative Python Structure):
zanzibar_analytics/ ├── data_ingestion/ │   ├── __init__.py │   ├── sierra_chart_loader.py  # Handles SCID/CSV parsing (Task 1.A, 2.A) │   ├── pms_parser.py           # Handles narrative parsing (Task 1.C, 2.B) │   ├── acap_connector.py       # Handles ACAP API connection (Task 1.B, 2.C) │   └── base_ingestor.py ├── data_management/ │   ├── __init__.py │   ├── db_interface.py         # Abstract interface for DBs │   ├── timeseries_db.py        # Implementation for InfluxDB/TimescaleDB │   ├── metadata_db.py          # Implementation for Postgres/other │   └── validator.py            # Data cleaning and validation rules ├── analysis_engine/ │   ├── __init__.py │   ├── liquidity/              # Phase 2 Modules │   │   ├── sweep_detector.py │   │   └── imbalance_analyzer.py │   ├── structure/              # Phase 3 Modules │   │   ├── wyckoff_engine.py │   │   └── smc_poi_engine.py │   ├── features/               # Phase 4 Modules │   │   ├── factor_calculator.py # TA-Lib, custom features │   │   └── feature_pipeline.py │   └── vwap_engine.py          # Phase 2 │   └── correlation_engine.py ├── ml_validation/              # Phase 4 Modules │   ├── __init__.py │   ├── model_trainer.py        # Wraps scikit-learn, LightGBM, etc. │   ├── validator.py            # Implements IC, DSR, BayesSR calcs │   ├── pms_evaluator.py        # Tracks PMS effectiveness │   └── interpreter.py          # SHAP integration ├── risk_portfolio/             # Phase 5 Modules │   ├── __init__.py │   ├── volatility_model.py     # GARCH, ML-based vol forecast │   ├── position_sizer.py       # Kelly Criterion implementation │   └── risk_calculator.py      # VaR/CVaR calculations ├── orchestration/              # Core Control │   ├── __init__.py │   ├── orchestrator.py         # Main control flow (Copilot logic) │   ├── config_manager.py       # Loads/manages YAML/JSON configs │   ├── logger.py               # Centralized logging setup │   └── scheduler.py            # Handles timed tasks (if needed) ├── execution_output/ │   ├── __init__.py │   ├── signal_processor.py     # Aggregates, filters signals │   ├── execution_adapters/ │   │   └── mt5_adapter.py      # Interface to MT5 (via Python API/other) │   └── reporting_interface.py  # API/service for dashboards/CLI ├── utils/                      # Common utilities │   └── ... ├── config/                     # Configuration files (YAML, JSON) │   └── ... ├── scripts/                    # Startup, maintenance scripts │   └── run_zanzibar.py ├── tests/                      # Unit and integration tests │   └── ... └── main.py                     # Entry point 
4. Data Flow:
	1	Raw Data Acquisition: sierra_chart_loader pulls Tick/DOM data. pms_parser receives raw text. acap_connector fetches status/market data.
	2	Ingestion & Structuring: Loaders/parsers clean, structure (e.g., PMS to JSON), and timestamp data.
	3	Storage & Validation: validator checks data integrity. db_interface routes validated data to appropriate databases (TimeSeries DB for market data, Metadata DB for PMS/config).
	4	Feature Generation: analysis_engine components read from TimeSeries DB, calculate liquidity metrics, structure events, POIs, and technical factors. Results are stored back in TimeSeries DB or passed directly.
	5	Narrative Integration: orchestrator fetches latest PMS bias from Metadata DB during analysis context building.
	6	Model Training/Prediction: ml_validation.model_trainer reads features/targets from TimeSeries DB, trains models (offline or periodically online), stores models. Prediction uses loaded models on new features.
	7	Signal Generation: Raw signals (e.g., POI hit, ML prediction crosses threshold) are generated by analysis/ML engines.
	8	Risk Overlay: risk_portfolio components calculate required margins, optimal position sizes (Kelly), and risk metrics (VaR/CVaR) based on current market data/volatility forecasts.
	9	Signal Aggregation: signal_processor combines raw signals, PMS bias, risk parameters, and filtering rules to produce a final, sized trade signal.
	10	Execution: execution_adapter translates the signal into an MT5-compatible order and sends it to the prop firm platform.
	11	Feedback Loop: Execution status, fills, and PnL are received back via the adapter, logged by logger, and stored via db_interface for performance tracking and PMS verification.
	12	Monitoring & Reporting: reporting_interface serves data from databases and logs to the UI/Dashboard/CLI.
5. Technology Stack Refinement:
	•	Time-Series Database: TimescaleDB (PostgreSQL extension) is a strong contender. It offers SQL familiarity, mature tooling, good performance for time-series queries, and can potentially store relational metadata (PMS, config) in the same instance, simplifying the stack compared to managing separate InfluxDB and Postgres. InfluxDB remains a viable alternative if NoSQL is preferred.
	•	Messaging/Task Queue (Optional but Recommended for Scale): For decoupling components, especially between analysis and execution, or for handling asynchronous tasks (like model training), consider Redis (with RQ or Celery) or RabbitMQ/Kafka if inter-service communication becomes complex. Start simple, add if needed.
	•	API Framework (for Reporting/Control): If building a web dashboard, FastAPI is a modern, high-performance choice for the reporting_interface.
	•	NLP Library (PMS): Start with spaCy for Polish support, entity recognition, and dependency parsing. More advanced transformers (Hugging Face) could be used later if needed.
	•	Execution Adapter (MT5): Investigate the official MetaTrader5 Python package. If insufficient, might need custom solutions (e.g., ZeroMQ bridge, file-based communication with an MT5 EA – requires care).
6. Key Design Considerations:
	•	Data Schema: Define clear, versioned schemas for database tables and inter-component messages.
	•	API Design: Internal APIs (like DB interfaces, reporting) should be well-defined.
	•	Configuration: Centralize configuration (database connections, model paths, strategy parameters, API keys) using YAML/JSON files managed by config_manager. Use environment variables for secrets.
	•	Error Handling: Implement robust error handling, logging, and potential retry mechanisms, especially for external API calls and data ingestion.
	•	State Management: The orchestrator needs to manage the state of the analysis pipeline effectively.
	•	Backtesting vs. Live: Design components to work in both modes where possible (e.g., data loaders reading historical vs. real-time feeds). Abstract away the data source.
7. Team Workflow:
	•	Data Eng: Owns ingestion pipelines, database schemas/performance, data validation. Provides clean data interfaces.
	•	Quant/ML: Develops core analysis logic, features, ML models, validation metrics. Consumes data via DB interfaces.
	•	Core/DevOps: Manages orchestration, core framework, execution adapters, infrastructure, CI/CD, monitoring. Ensures components integrate.
	•	Narrative/PMS: Focuses on the PMS parser, verification logic, and potentially UI elements for narrative review.
This detailed architecture provides a solid blueprint. The next step would be to drill down into specific interfaces (e.g., the exact structure of the data passed between the Analysis Engine and ML Engine) and start implementing the Phase 1 components according to this design.
